{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/javierarenas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/javierarenas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/javierarenas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodetoascii(text):\n",
    "    TEXT = (text.\n",
    "    \t\treplace('â\\x80\\x99', \"'\").\n",
    "            replace('â\\xc3\\xa9', 'e').\n",
    "            replace('â\\x80\\x90', '-').\n",
    "            replace('â\\x80\\x91', '-').\n",
    "            replace('â\\x80\\x92', '-').\n",
    "            replace('â\\x80\\x93', '-').\n",
    "            replace('â\\x80\\x94', '-').\n",
    "            replace('â\\x80\\x94', '-').\n",
    "            replace('â\\x80\\x98', \"'\").\n",
    "            replace('â\\x80\\x9b', \"'\").\n",
    "            replace('â\\x80\\x9c', '\"').\n",
    "            replace('â\\x80\\x9c', '\"').\n",
    "            replace('â\\x80\\x9d', '\"').\n",
    "            replace('â\\x80\\x9e', '\"').\n",
    "            replace('â\\x80\\x9f', '\"').\n",
    "            replace('â\\x80\\xa6', '...').#\n",
    "            replace('â\\x80\\xb2', \"'\").\n",
    "            replace('â\\x80\\xb3', \"'\").\n",
    "            replace('â\\x80\\xb4', \"'\").\n",
    "            replace('â\\x80\\xb5', \"'\").\n",
    "            replace('â\\x80\\xb6', \"'\").\n",
    "            replace('â\\x80\\xb7', \"'\").\n",
    "            replace('â\\x81\\xba', \"+\").\n",
    "            replace('â\\x81\\xbb', \"-\").\n",
    "            replace('â\\x81\\xbc', \"=\").\n",
    "            replace('â\\x81\\xbd', \"(\").\n",
    "            replace('â\\x81\\xbe', \")\").\n",
    "            replace('\\'', '')\n",
    "           )\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.nyt_dump\n",
    "coll = db.articles\n",
    "\n",
    "documents = ['\\n'.join(article['content']) for article in coll.find()]\n",
    "\n",
    "articles = []\n",
    "\n",
    "for i,document in enumerate(coll.find(projection={'content':1})):\n",
    "    \n",
    "    articles.append(document.get('content'))\n",
    "    articles[i] = ''.join(articles[i])\n",
    "    articles[i] = unicodetoascii(articles[i])\n",
    "    articles[i] = re.sub('\\n','',articles[i])\n",
    "    articles[i] = re.sub('\\s ',' ',articles[i])\n",
    "    articles[i] = remove_accents(articles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "tokens = [word_tokenize(article) for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lower = [[word.lower() for word in sent]\n",
    "                 for sent in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "punctuation_ = set(string.punctuation)\n",
    "\n",
    "special = set(\"',``,''\".split(','))\n",
    "\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_ and not w in special])\n",
    "\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "tokens_porter = [[porter.stem(token) for token in art] for art in tokens_filtered]\n",
    "tokens_snow = [[snowball.stem(token) for token in art] for art in tokens_filtered]\n",
    "tokens_wordnet = [[wordnet.lemmatize(token) for token in art] for art in tokens_filtered]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'man', 'phone', 'said', 'still', 'come', 'tonight', 'took', 'moment', 'realiz']\n",
      "['hey', 'man', 'phone', 'said', 'still', 'come', 'tonight', 'took', 'moment', 'realiz']\n",
      "['hey', 'man', 'phone', 'said', 'still', 'coming', 'tonight', 'took', 'moment', 'realize']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_porter[0][0:10])\n",
    "print(tokens_snow[0][0:10])\n",
    "print(tokens_wordnet[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'll use tokens wordnet since it takes the context of the sentence and makes it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for tokens in tokens_wordnet:\n",
    "    for word in tokens:\n",
    "        \n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "\n",
    "    vocab_dict.update({word:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_matrix = np.empty([len(tokens_wordnet), len(vocab_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 36118)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "term_occ = list(map(lambda art : Counter(art), tokens_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,terms in enumerate(term_occ):\n",
    "    for j,vocab in enumerate(vocab_dict):\n",
    "    \n",
    "        vocab_matrix[i,j] = term_occ[i][vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docc_freq = []\n",
    "\n",
    "for i in range(vocab_matrix.shape[1]):\n",
    "    \n",
    "    res = len(np.where(vocab_matrix[:,i] > 0)[0])\n",
    "    docc_freq.append(res/len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "term_freq = np.empty([vocab_matrix.shape[0], vocab_matrix.shape[1]])\n",
    "\n",
    "for i in range(vocab_matrix.shape[0]):\n",
    "    \n",
    "    term_freq[i,:] = vocab_matrix[i,:] / len(tokens_wordnet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = term_freq * np.log(1/np.array(docc_freq).reshape(1,len(docc_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 36118)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tf_idf.shape[0]):\n",
    "\n",
    "    tf_idf[i,:] = tf_idf[i,:] / np.linalg.norm(tf_idf[i,:],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "word_counts = vect.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = word_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docs):\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    \n",
    "    tokens_wordnet = [wordnet.lemmatize(token) for token in word_tokenize(docs.lower())]\n",
    "    \n",
    "    return tokens_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english', tokenizer = tokenize)\n",
    "word_counts = vect.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn count of 'dinner': 2\n",
      "my count of 'dinner': 2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"sklearn count of 'dinner':\", word_counts[0, words.index('dinner')])\n",
    "print(\"my count of 'dinner':\", vocab_matrix[0, 21804])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn tfidf of 'dinner': 0.05596800166849005\n",
      "my tfidf of 'dinner': 0.058522997606060646\n"
     ]
    }
   ],
   "source": [
    "words_tfidf = tfidfvect.get_feature_names()\n",
    "print(\"sklearn tfidf of 'dinner':\", tfidf_vectorized[0, words_tfidf.index('dinner')])\n",
    "print(\"my tfidf of 'dinner':\", tf_idf[0, 21804])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
