{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Implementing Tree Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the build_forest method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Implement Classification and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TreeNode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c9f6192e5c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTreeNode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TreeNode'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from TreeNode import TreeNode\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_features, impurity_criterion='entropy'):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "            \n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "        \n",
    "        # by default give features index names\n",
    "        self.feature_names = np.arange(X.shape[1])\n",
    "\n",
    "        # if X is a dataframe, set use the column names as feature names\n",
    "        if type(X)==pd.core.frame.DataFrame:\n",
    "            self.feature_names = list(X.columns)\n",
    "            X = X.values\n",
    "        \n",
    "        # if feature_names is explicitly set, override default feature names\n",
    "        if feature_names is not None:\n",
    "            if len(feature_names) != X.shape[1]:\n",
    "                raise ValueError(f\"feature_names has {len(feature_names)} items but X has {X.shape[1]} columns\")\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # Create True/False array of whether the variable is categorical\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) \n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "\n",
    "        node = TreeNode()\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            node.leaf = True\n",
    "            node.classes = Counter(y)\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "        else:\n",
    "            X1, y1, X2, y2 = splits\n",
    "            node.column = index\n",
    "            node.name = self.feature_names[index]\n",
    "            node.value = value\n",
    "            node.categorical = self.categorical[index]\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        for cl in np.unique(y):\n",
    "            prob = np.sum(y == cl) / float(len(y))\n",
    "            total += prob * math.log(prob)\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        for cl in np.unique(y):\n",
    "            prob = np.sum(y == cl) / float(len(y))\n",
    "            total += prob ** 2\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        >>> X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "\n",
    "        split_col = X[:, split_index]\n",
    "        if self.categorical[split_index]:\n",
    "            mask = split_col == split_value\n",
    "        else:\n",
    "            mask = split_col < split_value\n",
    "        return X[mask], y[mask], X[~mask], y[~mask]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "\n",
    "        total = self.impurity_criterion(y)\n",
    "        for y_split in (y1, y2):\n",
    "            ent = self.impurity_criterion(y_split)\n",
    "            total -= len(y_split) * ent / float(len(y))\n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        >>> index, value, splits = self._choose_split_index(X, y)\n",
    "        >>> X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        max_gain = 0\n",
    "        fcols = np.random.choice(range(len(X.shape[1])), self.num_features)\n",
    "        X = X[:,fcols]\n",
    "        for i in range(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            for val in values:\n",
    "                temp_splits = self._make_split(X[:, fcols], y, i, val)\n",
    "                X1, y1, X2, y2 = temp_splits\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                if gain > max_gain:\n",
    "                    max_gain = gain\n",
    "                    split_index, split_value = i, val\n",
    "                    splits = temp_splits\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        return np.array([self.root.predict_one(row) for row in X])\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree.\n",
    "        '''\n",
    "        return str(self.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DecisionTree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-804e9e16671f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDecisionTree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DecisionTree'"
     ]
    }
   ],
   "source": [
    "from DecisionTree import DecisionTree\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomForest(object):\n",
    "    '''A Random Forest class'''\n",
    "\n",
    "    def __init__(self, num_trees, num_features):\n",
    "        '''\n",
    "           num_trees:  number of trees to create in the forest:\n",
    "        num_features:  the number of features to consider when choosing the\n",
    "                           best split for each node of the decision trees\n",
    "        '''\n",
    "        self.num_trees = num_trees\n",
    "        self.num_features = num_features\n",
    "        self.forest = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X:  two dimensional numpy array representing feature matrix\n",
    "                for test data\n",
    "        y:  numpy array representing labels for test data\n",
    "        '''\n",
    "        self.forest = self.build_forest(X, y, self.num_trees,\n",
    "                                        self.num_features)\n",
    "\n",
    "    def build_forest(self, X, y, num_trees, num_features):\n",
    "        '''\n",
    "        Return a list of num_trees DecisionTrees built using bootstrap samples\n",
    "        and only considering num_features features at each branch.\n",
    "        '''\n",
    "        r = []\n",
    "        dt = DecisionTree(num_features)\n",
    "        \n",
    "        \n",
    "        for _ in range(num_trees):\n",
    "            \n",
    "            bootstrap_sample = np.random.choice(range(len(X)), len(X), True)\n",
    "            bootstrap_cols = np.random.choice(X.columns, n_features, False)\n",
    "            r.append(dt.fit(X[bootstrap_sample,:], y[bootstrap_sample]))\n",
    "            \n",
    "        return r\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Return a numpy array of the labels predicted for the given test data.\n",
    "        '''\n",
    "        res = np.empty(len(X.shape[0]))\n",
    "        \n",
    "        for n,i in enumerate(range(X.shape[0])):\n",
    "            \n",
    "            r = np.array([self.root.predict_one(row)])\n",
    "            \n",
    "            res[n] = stats.mode(r)[0][0]\n",
    "                    \n",
    "        return res                      \n",
    "                \n",
    "\n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "        Return the accuracy of the Random Forest for the given test data and\n",
    "        labels.\n",
    "        '''\n",
    "        \n",
    "        res = self.predict(X, y)\n",
    "        proportion = np.empty(range(np.unique(y)))\n",
    "        \n",
    "        for n,i in enumerate(np.unique(res)):\n",
    "            \n",
    "            proportion[n] = np.sum(res == i) / len(y)\n",
    "        \n",
    "        return np.mean(proportion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "\n",
    "    # This is for visualizing your tree. You don't need to look into this code.\n",
    "    def as_string(self, level=0, prefix=\"\"):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - level: int (amount to indent)\n",
    "        OUTPUT:\n",
    "            - prefix: str (to start the line with)\n",
    "        Return a string representation of the tree rooted at this node.\n",
    "        '''\n",
    "        result = \"\"\n",
    "        if prefix:\n",
    "            indent = \"  |   \" * (level - 1) + \"  |-> \"\n",
    "            result += indent + prefix + \"\\n\"\n",
    "        indent = \"  |   \" * level\n",
    "        result += indent + \"  \" + str(self.name) + \"\\n\"\n",
    "        if not self.leaf:\n",
    "            if self.categorical:\n",
    "                left_key = str(self.value)\n",
    "                right_key = \"no \" + str(self.value)\n",
    "            else:\n",
    "                left_key = \"< \" + str(self.value)\n",
    "                right_key = \">= \" + str(self.value)\n",
    "            result += self.left.as_string(level + 1, left_key + \":\")\n",
    "            result += self.right.as_string(level + 1, right_key + \":\")\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.as_string().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
